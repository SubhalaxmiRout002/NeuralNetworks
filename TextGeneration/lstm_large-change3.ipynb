{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Recurrent Network for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARY IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET AND CREATE MAP OF UNIQUE CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  6905\n",
      "Total Vocab:  26\n"
     ]
    }
   ],
   "source": [
    "# Load the text corpus\n",
    "filename = \"Cat_in_the_Hat.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    "# Convert all the text to lowercase\n",
    "raw_text = \"\".join(v for v in raw_text if v not in string.punctuation).lower()\n",
    "\n",
    "\n",
    "# Create a map that maps each unique character in the text to a unique integer value\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Display the total number of characters (n_chars) and the vocabulary (the number of unique characters)\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE TRAINING PATTERNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  6805\n"
     ]
    }
   ],
   "source": [
    "# Create the patterns to be used for training\n",
    "seq_length = 100 # fixed length sliding window for training pattern\n",
    "dataX = [] # input sequences\n",
    "dataY = [] # outputs\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM DATA TO BE SUITABLE FOR KERAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dataX to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# Rescale integers mapped to characters to the range 0-to-1 to accommodate learning using sigmoid function\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD THE LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequential model containing 1 LSTM layers and 1 Dropout layers, followed by a dense output layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model using the Adam optimizer and categorical crossentropy for the loss function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Define the checkpoint; i.e., saved past state of the model\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.8787\n",
      "Epoch 00001: loss improved from inf to 2.87867, saving model to weights-improvement-01-2.8787-bigger.hdf5\n",
      "107/107 [==============================] - 46s 397ms/step - loss: 2.8787\n",
      "Epoch 2/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.8385\n",
      "Epoch 00002: loss improved from 2.87867 to 2.83848, saving model to weights-improvement-02-2.8385-bigger.hdf5\n",
      "107/107 [==============================] - 44s 415ms/step - loss: 2.8385\n",
      "Epoch 3/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.8256\n",
      "Epoch 00003: loss improved from 2.83848 to 2.82563, saving model to weights-improvement-03-2.8256-bigger.hdf5\n",
      "107/107 [==============================] - 44s 407ms/step - loss: 2.8256\n",
      "Epoch 4/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.7766\n",
      "Epoch 00004: loss improved from 2.82563 to 2.77657, saving model to weights-improvement-04-2.7766-bigger.hdf5\n",
      "107/107 [==============================] - 43s 404ms/step - loss: 2.7766\n",
      "Epoch 5/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.6920\n",
      "Epoch 00005: loss improved from 2.77657 to 2.69205, saving model to weights-improvement-05-2.6920-bigger.hdf5\n",
      "107/107 [==============================] - 42s 390ms/step - loss: 2.6920\n",
      "Epoch 6/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.6077\n",
      "Epoch 00006: loss improved from 2.69205 to 2.60766, saving model to weights-improvement-06-2.6077-bigger.hdf5\n",
      "107/107 [==============================] - 42s 391ms/step - loss: 2.6077\n",
      "Epoch 7/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.4950\n",
      "Epoch 00007: loss improved from 2.60766 to 2.49504, saving model to weights-improvement-07-2.4950-bigger.hdf5\n",
      "107/107 [==============================] - 42s 391ms/step - loss: 2.4950\n",
      "Epoch 8/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.3972\n",
      "Epoch 00008: loss improved from 2.49504 to 2.39724, saving model to weights-improvement-08-2.3972-bigger.hdf5\n",
      "107/107 [==============================] - 43s 397ms/step - loss: 2.3972\n",
      "Epoch 9/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.2924\n",
      "Epoch 00009: loss improved from 2.39724 to 2.29245, saving model to weights-improvement-09-2.2924-bigger.hdf5\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 2.2924\n",
      "Epoch 10/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.1952\n",
      "Epoch 00010: loss improved from 2.29245 to 2.19521, saving model to weights-improvement-10-2.1952-bigger.hdf5\n",
      "107/107 [==============================] - 42s 391ms/step - loss: 2.1952\n",
      "Epoch 11/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 2.0974\n",
      "Epoch 00011: loss improved from 2.19521 to 2.09736, saving model to weights-improvement-11-2.0974-bigger.hdf5\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 2.0974\n",
      "Epoch 12/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.9976\n",
      "Epoch 00012: loss improved from 2.09736 to 1.99761, saving model to weights-improvement-12-1.9976-bigger.hdf5\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 1.9976\n",
      "Epoch 13/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.8822\n",
      "Epoch 00013: loss improved from 1.99761 to 1.88221, saving model to weights-improvement-13-1.8822-bigger.hdf5\n",
      "107/107 [==============================] - 42s 394ms/step - loss: 1.8822\n",
      "Epoch 14/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.7771\n",
      "Epoch 00014: loss improved from 1.88221 to 1.77709, saving model to weights-improvement-14-1.7771-bigger.hdf5\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 1.7771\n",
      "Epoch 15/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.6823\n",
      "Epoch 00015: loss improved from 1.77709 to 1.68232, saving model to weights-improvement-15-1.6823-bigger.hdf5\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 1.6823\n",
      "Epoch 16/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.5762\n",
      "Epoch 00016: loss improved from 1.68232 to 1.57616, saving model to weights-improvement-16-1.5762-bigger.hdf5\n",
      "107/107 [==============================] - 44s 416ms/step - loss: 1.5762\n",
      "Epoch 17/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.4726\n",
      "Epoch 00017: loss improved from 1.57616 to 1.47256, saving model to weights-improvement-17-1.4726-bigger.hdf5\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 1.4726\n",
      "Epoch 18/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.3699\n",
      "Epoch 00018: loss improved from 1.47256 to 1.36990, saving model to weights-improvement-18-1.3699-bigger.hdf5\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 1.3699\n",
      "Epoch 19/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.2686\n",
      "Epoch 00019: loss improved from 1.36990 to 1.26855, saving model to weights-improvement-19-1.2686-bigger.hdf5\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 1.2686\n",
      "Epoch 20/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.1584\n",
      "Epoch 00020: loss improved from 1.26855 to 1.15842, saving model to weights-improvement-20-1.1584-bigger.hdf5\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 1.1584\n",
      "Epoch 21/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.0863\n",
      "Epoch 00021: loss improved from 1.15842 to 1.08630, saving model to weights-improvement-21-1.0863-bigger.hdf5\n",
      "107/107 [==============================] - 42s 394ms/step - loss: 1.0863\n",
      "Epoch 22/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.9939\n",
      "Epoch 00022: loss improved from 1.08630 to 0.99394, saving model to weights-improvement-22-0.9939-bigger.hdf5\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 0.9939\n",
      "Epoch 23/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.9000\n",
      "Epoch 00023: loss improved from 0.99394 to 0.90003, saving model to weights-improvement-23-0.9000-bigger.hdf5\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.9000\n",
      "Epoch 24/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.8274\n",
      "Epoch 00024: loss improved from 0.90003 to 0.82743, saving model to weights-improvement-24-0.8274-bigger.hdf5\n",
      "107/107 [==============================] - 152s 1s/step - loss: 0.8274\n",
      "Epoch 25/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7561\n",
      "Epoch 00025: loss improved from 0.82743 to 0.75610, saving model to weights-improvement-25-0.7561-bigger.hdf5\n",
      "107/107 [==============================] - 42s 392ms/step - loss: 0.7561\n",
      "Epoch 26/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6831\n",
      "Epoch 00026: loss improved from 0.75610 to 0.68308, saving model to weights-improvement-26-0.6831-bigger.hdf5\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.6831\n",
      "Epoch 27/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6196\n",
      "Epoch 00027: loss improved from 0.68308 to 0.61958, saving model to weights-improvement-27-0.6196-bigger.hdf5\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 0.6196\n",
      "Epoch 28/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5657\n",
      "Epoch 00028: loss improved from 0.61958 to 0.56565, saving model to weights-improvement-28-0.5657-bigger.hdf5\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 0.5657\n",
      "Epoch 29/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5191\n",
      "Epoch 00029: loss improved from 0.56565 to 0.51908, saving model to weights-improvement-29-0.5191-bigger.hdf5\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 0.5191\n",
      "Epoch 30/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4558\n",
      "Epoch 00030: loss improved from 0.51908 to 0.45584, saving model to weights-improvement-30-0.4558-bigger.hdf5\n",
      "107/107 [==============================] - 43s 400ms/step - loss: 0.4558\n",
      "Epoch 31/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.4131\n",
      "Epoch 00031: loss improved from 0.45584 to 0.41310, saving model to weights-improvement-31-0.4131-bigger.hdf5\n",
      "107/107 [==============================] - 42s 394ms/step - loss: 0.4131\n",
      "Epoch 32/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3829\n",
      "Epoch 00032: loss improved from 0.41310 to 0.38286, saving model to weights-improvement-32-0.3829-bigger.hdf5\n",
      "107/107 [==============================] - 43s 397ms/step - loss: 0.3829\n",
      "Epoch 33/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3415\n",
      "Epoch 00033: loss improved from 0.38286 to 0.34152, saving model to weights-improvement-33-0.3415-bigger.hdf5\n",
      "107/107 [==============================] - 42s 397ms/step - loss: 0.3415\n",
      "Epoch 34/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.3135\n",
      "Epoch 00034: loss improved from 0.34152 to 0.31351, saving model to weights-improvement-34-0.3135-bigger.hdf5\n",
      "107/107 [==============================] - 42s 397ms/step - loss: 0.3135\n",
      "Epoch 35/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.2850\n",
      "Epoch 00035: loss improved from 0.31351 to 0.28498, saving model to weights-improvement-35-0.2850-bigger.hdf5\n",
      "107/107 [==============================] - 42s 396ms/step - loss: 0.2850\n",
      "Epoch 36/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.2643\n",
      "Epoch 00036: loss improved from 0.28498 to 0.26426, saving model to weights-improvement-36-0.2643-bigger.hdf5\n",
      "107/107 [==============================] - 42s 394ms/step - loss: 0.2643\n",
      "Epoch 37/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.2324\n",
      "Epoch 00037: loss improved from 0.26426 to 0.23240, saving model to weights-improvement-37-0.2324-bigger.hdf5\n",
      "107/107 [==============================] - 42s 396ms/step - loss: 0.2324\n",
      "Epoch 38/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.2094\n",
      "Epoch 00038: loss improved from 0.23240 to 0.20939, saving model to weights-improvement-38-0.2094-bigger.hdf5\n",
      "107/107 [==============================] - 43s 402ms/step - loss: 0.2094\n",
      "Epoch 39/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1930\n",
      "Epoch 00039: loss improved from 0.20939 to 0.19302, saving model to weights-improvement-39-0.1930-bigger.hdf5\n",
      "107/107 [==============================] - 43s 403ms/step - loss: 0.1930\n",
      "Epoch 40/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00040: loss improved from 0.19302 to 0.18759, saving model to weights-improvement-40-0.1876-bigger.hdf5\n",
      "107/107 [==============================] - 45s 423ms/step - loss: 0.1876\n",
      "Epoch 41/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1639\n",
      "Epoch 00041: loss improved from 0.18759 to 0.16394, saving model to weights-improvement-41-0.1639-bigger.hdf5\n",
      "107/107 [==============================] - 45s 422ms/step - loss: 0.1639\n",
      "Epoch 42/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1451\n",
      "Epoch 00042: loss improved from 0.16394 to 0.14505, saving model to weights-improvement-42-0.1451-bigger.hdf5\n",
      "107/107 [==============================] - 45s 421ms/step - loss: 0.1451\n",
      "Epoch 43/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1442\n",
      "Epoch 00043: loss improved from 0.14505 to 0.14416, saving model to weights-improvement-43-0.1442-bigger.hdf5\n",
      "107/107 [==============================] - 45s 419ms/step - loss: 0.1442\n",
      "Epoch 44/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1398\n",
      "Epoch 00044: loss improved from 0.14416 to 0.13980, saving model to weights-improvement-44-0.1398-bigger.hdf5\n",
      "107/107 [==============================] - 42s 394ms/step - loss: 0.1398\n",
      "Epoch 45/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1325\n",
      "Epoch 00045: loss improved from 0.13980 to 0.13252, saving model to weights-improvement-45-0.1325-bigger.hdf5\n",
      "107/107 [==============================] - 43s 403ms/step - loss: 0.1325\n",
      "Epoch 46/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1316\n",
      "Epoch 00046: loss improved from 0.13252 to 0.13164, saving model to weights-improvement-46-0.1316-bigger.hdf5\n",
      "107/107 [==============================] - 42s 393ms/step - loss: 0.1316\n",
      "Epoch 47/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1145\n",
      "Epoch 00047: loss improved from 0.13164 to 0.11449, saving model to weights-improvement-47-0.1145-bigger.hdf5\n",
      "107/107 [==============================] - 43s 398ms/step - loss: 0.1145\n",
      "Epoch 48/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 00048: loss improved from 0.11449 to 0.10354, saving model to weights-improvement-48-0.1035-bigger.hdf5\n",
      "107/107 [==============================] - 42s 396ms/step - loss: 0.1035\n",
      "Epoch 49/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00049: loss improved from 0.10354 to 0.10205, saving model to weights-improvement-49-0.1021-bigger.hdf5\n",
      "107/107 [==============================] - 42s 395ms/step - loss: 0.1021\n",
      "Epoch 50/50\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00050: loss improved from 0.10205 to 0.09913, saving model to weights-improvement-50-0.0991-bigger.hdf5\n",
      "107/107 [==============================] - 42s 396ms/step - loss: 0.0991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83b426a610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
