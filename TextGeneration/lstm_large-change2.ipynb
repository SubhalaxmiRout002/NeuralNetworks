{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Recurrent Network for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARY IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET AND CREATE MAP OF UNIQUE CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  7353\n",
      "Total Vocab:  32\n"
     ]
    }
   ],
   "source": [
    "# Load the text corpus\n",
    "filename = \"Cat_in_the_Hat.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    "# Convert all the text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# Create a map that maps each unique character in the text to a unique integer value\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Display the total number of characters (n_chars) and the vocabulary (the number of unique characters)\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE TRAINING PATTERNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  7328\n"
     ]
    }
   ],
   "source": [
    "# Create the patterns to be used for training\n",
    "seq_length = 25 # fixed length sliding window for training pattern\n",
    "dataX = [] # input sequences\n",
    "dataY = [] # outputs\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM DATA TO BE SUITABLE FOR KERAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dataX to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# Rescale integers mapped to characters to the range 0-to-1 to accommodate learning using sigmoid function\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD THE LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequential model containing 1 LSTM layers and 1 Dropout layers, followed by a dense output layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model using the Adam optimizer and categorical crossentropy for the loss function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Define the checkpoint; i.e., saved past state of the model\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 3.0253\n",
      "Epoch 00001: loss improved from inf to 3.02529, saving model to weights-improvement-01-3.0253-bigger.hdf5\n",
      "115/115 [==============================] - 14s 101ms/step - loss: 3.0253\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.9860\n",
      "Epoch 00002: loss improved from 3.02529 to 2.98604, saving model to weights-improvement-02-2.9860-bigger.hdf5\n",
      "115/115 [==============================] - 12s 102ms/step - loss: 2.9860\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.9419\n",
      "Epoch 00003: loss improved from 2.98604 to 2.94188, saving model to weights-improvement-03-2.9419-bigger.hdf5\n",
      "115/115 [==============================] - 12s 103ms/step - loss: 2.9419\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.8195\n",
      "Epoch 00004: loss improved from 2.94188 to 2.81951, saving model to weights-improvement-04-2.8195-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 2.8195\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.6823\n",
      "Epoch 00005: loss improved from 2.81951 to 2.68230, saving model to weights-improvement-05-2.6823-bigger.hdf5\n",
      "115/115 [==============================] - 12s 103ms/step - loss: 2.6823\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.5505\n",
      "Epoch 00006: loss improved from 2.68230 to 2.55053, saving model to weights-improvement-06-2.5505-bigger.hdf5\n",
      "115/115 [==============================] - 12s 104ms/step - loss: 2.5505\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.4327\n",
      "Epoch 00007: loss improved from 2.55053 to 2.43269, saving model to weights-improvement-07-2.4327-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 2.4327\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.3282\n",
      "Epoch 00008: loss improved from 2.43269 to 2.32823, saving model to weights-improvement-08-2.3282-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 2.3282\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.2360\n",
      "Epoch 00009: loss improved from 2.32823 to 2.23603, saving model to weights-improvement-09-2.2360-bigger.hdf5\n",
      "115/115 [==============================] - 12s 104ms/step - loss: 2.2360\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.1320\n",
      "Epoch 00010: loss improved from 2.23603 to 2.13204, saving model to weights-improvement-10-2.1320-bigger.hdf5\n",
      "115/115 [==============================] - 12s 104ms/step - loss: 2.1320\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 2.0211\n",
      "Epoch 00011: loss improved from 2.13204 to 2.02115, saving model to weights-improvement-11-2.0211-bigger.hdf5\n",
      "115/115 [==============================] - 12s 108ms/step - loss: 2.0211\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.9258\n",
      "Epoch 00012: loss improved from 2.02115 to 1.92577, saving model to weights-improvement-12-1.9258-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 1.9258\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.8230\n",
      "Epoch 00013: loss improved from 1.92577 to 1.82303, saving model to weights-improvement-13-1.8230-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 1.8230\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.7164\n",
      "Epoch 00014: loss improved from 1.82303 to 1.71638, saving model to weights-improvement-14-1.7164-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 1.7164\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.6080\n",
      "Epoch 00015: loss improved from 1.71638 to 1.60803, saving model to weights-improvement-15-1.6080-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 1.6080\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.5012\n",
      "Epoch 00016: loss improved from 1.60803 to 1.50123, saving model to weights-improvement-16-1.5012-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 1.5012\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.4012\n",
      "Epoch 00017: loss improved from 1.50123 to 1.40124, saving model to weights-improvement-17-1.4012-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 1.4012\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.3073\n",
      "Epoch 00018: loss improved from 1.40124 to 1.30732, saving model to weights-improvement-18-1.3073-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 1.3073\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.2134\n",
      "Epoch 00019: loss improved from 1.30732 to 1.21340, saving model to weights-improvement-19-1.2134-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 1.2134\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.1185\n",
      "Epoch 00020: loss improved from 1.21340 to 1.11854, saving model to weights-improvement-20-1.1185-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 1.1185\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 1.0271\n",
      "Epoch 00021: loss improved from 1.11854 to 1.02708, saving model to weights-improvement-21-1.0271-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 1.0271\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.9426\n",
      "Epoch 00022: loss improved from 1.02708 to 0.94259, saving model to weights-improvement-22-0.9426-bigger.hdf5\n",
      "115/115 [==============================] - 12s 107ms/step - loss: 0.9426\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.8550\n",
      "Epoch 00023: loss improved from 0.94259 to 0.85496, saving model to weights-improvement-23-0.8550-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.8550\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.7922\n",
      "Epoch 00024: loss improved from 0.85496 to 0.79219, saving model to weights-improvement-24-0.7922-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.7922\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.7074\n",
      "Epoch 00025: loss improved from 0.79219 to 0.70740, saving model to weights-improvement-25-0.7074-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.7074\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6564\n",
      "Epoch 00026: loss improved from 0.70740 to 0.65640, saving model to weights-improvement-26-0.6564-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.6564\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5940\n",
      "Epoch 00027: loss improved from 0.65640 to 0.59397, saving model to weights-improvement-27-0.5940-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.5940\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5336\n",
      "Epoch 00028: loss improved from 0.59397 to 0.53363, saving model to weights-improvement-28-0.5336-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.5336\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4857\n",
      "Epoch 00029: loss improved from 0.53363 to 0.48565, saving model to weights-improvement-29-0.4857-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.4857\n",
      "Epoch 30/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4372\n",
      "Epoch 00030: loss improved from 0.48565 to 0.43723, saving model to weights-improvement-30-0.4372-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.4372\n",
      "Epoch 31/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4011\n",
      "Epoch 00031: loss improved from 0.43723 to 0.40107, saving model to weights-improvement-31-0.4011-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.4011\n",
      "Epoch 32/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3614\n",
      "Epoch 00032: loss improved from 0.40107 to 0.36140, saving model to weights-improvement-32-0.3614-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.3614\n",
      "Epoch 33/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3253\n",
      "Epoch 00033: loss improved from 0.36140 to 0.32525, saving model to weights-improvement-33-0.3253-bigger.hdf5\n",
      "115/115 [==============================] - 12s 107ms/step - loss: 0.3253\n",
      "Epoch 34/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.3009\n",
      "Epoch 00034: loss improved from 0.32525 to 0.30088, saving model to weights-improvement-34-0.3009-bigger.hdf5\n",
      "115/115 [==============================] - 12s 108ms/step - loss: 0.3009\n",
      "Epoch 35/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2760\n",
      "Epoch 00035: loss improved from 0.30088 to 0.27598, saving model to weights-improvement-35-0.2760-bigger.hdf5\n",
      "115/115 [==============================] - 12s 104ms/step - loss: 0.2760\n",
      "Epoch 36/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2398\n",
      "Epoch 00036: loss improved from 0.27598 to 0.23980, saving model to weights-improvement-36-0.2398-bigger.hdf5\n",
      "115/115 [==============================] - 12s 107ms/step - loss: 0.2398\n",
      "Epoch 37/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2265\n",
      "Epoch 00037: loss improved from 0.23980 to 0.22654, saving model to weights-improvement-37-0.2265-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.2265\n",
      "Epoch 38/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00038: loss improved from 0.22654 to 0.20866, saving model to weights-improvement-38-0.2087-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.2087\n",
      "Epoch 39/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1921\n",
      "Epoch 00039: loss improved from 0.20866 to 0.19205, saving model to weights-improvement-39-0.1921-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.1921\n",
      "Epoch 40/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2166\n",
      "Epoch 00040: loss did not improve from 0.19205\n",
      "115/115 [==============================] - 12s 104ms/step - loss: 0.2166\n",
      "Epoch 41/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00041: loss improved from 0.19205 to 0.18591, saving model to weights-improvement-41-0.1859-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.1859\n",
      "Epoch 42/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1537\n",
      "Epoch 00042: loss improved from 0.18591 to 0.15371, saving model to weights-improvement-42-0.1537-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.1537\n",
      "Epoch 43/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1414\n",
      "Epoch 00043: loss improved from 0.15371 to 0.14138, saving model to weights-improvement-43-0.1414-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.1414\n",
      "Epoch 44/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1346\n",
      "Epoch 00044: loss improved from 0.14138 to 0.13463, saving model to weights-improvement-44-0.1346-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.1346\n",
      "Epoch 45/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1300\n",
      "Epoch 00045: loss improved from 0.13463 to 0.13002, saving model to weights-improvement-45-0.1300-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.1300\n",
      "Epoch 46/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00046: loss improved from 0.13002 to 0.12634, saving model to weights-improvement-46-0.1263-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.1263\n",
      "Epoch 47/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1293\n",
      "Epoch 00047: loss did not improve from 0.12634\n",
      "115/115 [==============================] - 12s 104ms/step - loss: 0.1293\n",
      "Epoch 48/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00048: loss did not improve from 0.12634\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.1312\n",
      "Epoch 49/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00049: loss improved from 0.12634 to 0.12623, saving model to weights-improvement-49-0.1262-bigger.hdf5\n",
      "115/115 [==============================] - 12s 106ms/step - loss: 0.1262\n",
      "Epoch 50/50\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 00050: loss improved from 0.12623 to 0.10600, saving model to weights-improvement-50-0.1060-bigger.hdf5\n",
      "115/115 [==============================] - 12s 105ms/step - loss: 0.1060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae8044b810>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
