{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Recurrent Network for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARY IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET AND CREATE MAP OF UNIQUE CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  7353\n",
      "Total Vocab:  32\n"
     ]
    }
   ],
   "source": [
    "# Load the text corpus\n",
    "filename = \"Cat_in_the_Hat.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    "# Convert all the text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# Create a map that maps each unique character in the text to a unique integer value\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Display the total number of characters (n_chars) and the vocabulary (the number of unique characters)\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE TRAINING PATTERNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  7253\n"
     ]
    }
   ],
   "source": [
    "# Create the patterns to be used for training\n",
    "seq_length = 100 # fixed length sliding window for training pattern\n",
    "dataX = [] # input sequences\n",
    "dataY = [] # outputs\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM DATA TO BE SUITABLE FOR KERAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dataX to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# Rescale integers mapped to characters to the range 0-to-1 to accommodate learning using sigmoid function\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD THE LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequential model containing 1 LSTM layers and 1 Dropout layers, followed by a dense output layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model using the Adam optimizer and categorical crossentropy for the loss function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Define the checkpoint; i.e., saved past state of the model\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 3.0351\n",
      "Epoch 00001: loss improved from inf to 3.03508, saving model to weights-improvement-01-3.0351-bigger.hdf5\n",
      "114/114 [==============================] - 18s 145ms/step - loss: 3.0351\n",
      "Epoch 2/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.9865\n",
      "Epoch 00002: loss improved from 3.03508 to 2.98648, saving model to weights-improvement-02-2.9865-bigger.hdf5\n",
      "114/114 [==============================] - 17s 150ms/step - loss: 2.9865\n",
      "Epoch 3/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.9755\n",
      "Epoch 00003: loss improved from 2.98648 to 2.97550, saving model to weights-improvement-03-2.9755-bigger.hdf5\n",
      "114/114 [==============================] - 25s 218ms/step - loss: 2.9755\n",
      "Epoch 4/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.9449\n",
      "Epoch 00004: loss improved from 2.97550 to 2.94491, saving model to weights-improvement-04-2.9449-bigger.hdf5\n",
      "114/114 [==============================] - 18s 159ms/step - loss: 2.9449\n",
      "Epoch 5/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.8591\n",
      "Epoch 00005: loss improved from 2.94491 to 2.85910, saving model to weights-improvement-05-2.8591-bigger.hdf5\n",
      "114/114 [==============================] - 17s 152ms/step - loss: 2.8591\n",
      "Epoch 6/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.7932\n",
      "Epoch 00006: loss improved from 2.85910 to 2.79322, saving model to weights-improvement-06-2.7932-bigger.hdf5\n",
      "114/114 [==============================] - 17s 153ms/step - loss: 2.7932\n",
      "Epoch 7/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.7442\n",
      "Epoch 00007: loss improved from 2.79322 to 2.74418, saving model to weights-improvement-07-2.7442-bigger.hdf5\n",
      "114/114 [==============================] - 18s 155ms/step - loss: 2.7442\n",
      "Epoch 8/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.6980\n",
      "Epoch 00008: loss improved from 2.74418 to 2.69796, saving model to weights-improvement-08-2.6980-bigger.hdf5\n",
      "114/114 [==============================] - 18s 154ms/step - loss: 2.6980\n",
      "Epoch 9/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.6320\n",
      "Epoch 00009: loss improved from 2.69796 to 2.63196, saving model to weights-improvement-09-2.6320-bigger.hdf5\n",
      "114/114 [==============================] - 18s 154ms/step - loss: 2.6320\n",
      "Epoch 10/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.5642\n",
      "Epoch 00010: loss improved from 2.63196 to 2.56421, saving model to weights-improvement-10-2.5642-bigger.hdf5\n",
      "114/114 [==============================] - 18s 155ms/step - loss: 2.5642\n",
      "Epoch 11/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.4879\n",
      "Epoch 00011: loss improved from 2.56421 to 2.48787, saving model to weights-improvement-11-2.4879-bigger.hdf5\n",
      "114/114 [==============================] - 20s 173ms/step - loss: 2.4879\n",
      "Epoch 12/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.4340\n",
      "Epoch 00012: loss improved from 2.48787 to 2.43398, saving model to weights-improvement-12-2.4340-bigger.hdf5\n",
      "114/114 [==============================] - 17s 153ms/step - loss: 2.4340\n",
      "Epoch 13/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.3788\n",
      "Epoch 00013: loss improved from 2.43398 to 2.37884, saving model to weights-improvement-13-2.3788-bigger.hdf5\n",
      "114/114 [==============================] - 18s 156ms/step - loss: 2.3788\n",
      "Epoch 14/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.3125\n",
      "Epoch 00014: loss improved from 2.37884 to 2.31247, saving model to weights-improvement-14-2.3125-bigger.hdf5\n",
      "114/114 [==============================] - 19s 159ms/step - loss: 2.3125\n",
      "Epoch 15/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.2610\n",
      "Epoch 00015: loss improved from 2.31247 to 2.26098, saving model to weights-improvement-15-2.2610-bigger.hdf5\n",
      "114/114 [==============================] - 18s 154ms/step - loss: 2.2610\n",
      "Epoch 16/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.1885\n",
      "Epoch 00016: loss improved from 2.26098 to 2.18848, saving model to weights-improvement-16-2.1885-bigger.hdf5\n",
      "114/114 [==============================] - 17s 152ms/step - loss: 2.1885\n",
      "Epoch 17/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.1293\n",
      "Epoch 00017: loss improved from 2.18848 to 2.12933, saving model to weights-improvement-17-2.1293-bigger.hdf5\n",
      "114/114 [==============================] - 18s 155ms/step - loss: 2.1293\n",
      "Epoch 18/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.0561\n",
      "Epoch 00018: loss improved from 2.12933 to 2.05607, saving model to weights-improvement-18-2.0561-bigger.hdf5\n",
      "114/114 [==============================] - 20s 172ms/step - loss: 2.0561\n",
      "Epoch 19/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.9750\n",
      "Epoch 00019: loss improved from 2.05607 to 1.97498, saving model to weights-improvement-19-1.9750-bigger.hdf5\n",
      "114/114 [==============================] - 17s 153ms/step - loss: 1.9750\n",
      "Epoch 20/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.9037\n",
      "Epoch 00020: loss improved from 1.97498 to 1.90368, saving model to weights-improvement-20-1.9037-bigger.hdf5\n",
      "114/114 [==============================] - 18s 155ms/step - loss: 1.9037\n",
      "Epoch 21/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.8160\n",
      "Epoch 00021: loss improved from 1.90368 to 1.81603, saving model to weights-improvement-21-1.8160-bigger.hdf5\n",
      "114/114 [==============================] - 17s 153ms/step - loss: 1.8160\n",
      "Epoch 22/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.7369\n",
      "Epoch 00022: loss improved from 1.81603 to 1.73693, saving model to weights-improvement-22-1.7369-bigger.hdf5\n",
      "114/114 [==============================] - 17s 152ms/step - loss: 1.7369\n",
      "Epoch 23/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.6499\n",
      "Epoch 00023: loss improved from 1.73693 to 1.64993, saving model to weights-improvement-23-1.6499-bigger.hdf5\n",
      "114/114 [==============================] - 17s 150ms/step - loss: 1.6499\n",
      "Epoch 24/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.5666\n",
      "Epoch 00024: loss improved from 1.64993 to 1.56661, saving model to weights-improvement-24-1.5666-bigger.hdf5\n",
      "114/114 [==============================] - 19s 164ms/step - loss: 1.5666\n",
      "Epoch 25/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.4864\n",
      "Epoch 00025: loss improved from 1.56661 to 1.48637, saving model to weights-improvement-25-1.4864-bigger.hdf5\n",
      "114/114 [==============================] - 21s 187ms/step - loss: 1.4864\n",
      "Epoch 26/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.4090\n",
      "Epoch 00026: loss improved from 1.48637 to 1.40899, saving model to weights-improvement-26-1.4090-bigger.hdf5\n",
      "114/114 [==============================] - 18s 156ms/step - loss: 1.4090\n",
      "Epoch 27/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.3199\n",
      "Epoch 00027: loss improved from 1.40899 to 1.31990, saving model to weights-improvement-27-1.3199-bigger.hdf5\n",
      "114/114 [==============================] - 19s 164ms/step - loss: 1.3199\n",
      "Epoch 28/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.2428\n",
      "Epoch 00028: loss improved from 1.31990 to 1.24280, saving model to weights-improvement-28-1.2428-bigger.hdf5\n",
      "114/114 [==============================] - 18s 154ms/step - loss: 1.2428\n",
      "Epoch 29/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.1742\n",
      "Epoch 00029: loss improved from 1.24280 to 1.17424, saving model to weights-improvement-29-1.1742-bigger.hdf5\n",
      "114/114 [==============================] - 18s 153ms/step - loss: 1.1742\n",
      "Epoch 30/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.0794\n",
      "Epoch 00030: loss improved from 1.17424 to 1.07939, saving model to weights-improvement-30-1.0794-bigger.hdf5\n",
      "114/114 [==============================] - 20s 173ms/step - loss: 1.0794\n",
      "Epoch 31/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.9985\n",
      "Epoch 00031: loss improved from 1.07939 to 0.99846, saving model to weights-improvement-31-0.9985-bigger.hdf5\n",
      "114/114 [==============================] - 17s 151ms/step - loss: 0.9985\n",
      "Epoch 32/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.9447\n",
      "Epoch 00032: loss improved from 0.99846 to 0.94473, saving model to weights-improvement-32-0.9447-bigger.hdf5\n",
      "114/114 [==============================] - 17s 148ms/step - loss: 0.9447\n",
      "Epoch 33/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.8705\n",
      "Epoch 00033: loss improved from 0.94473 to 0.87046, saving model to weights-improvement-33-0.8705-bigger.hdf5\n",
      "114/114 [==============================] - 17s 149ms/step - loss: 0.8705\n",
      "Epoch 34/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.8045\n",
      "Epoch 00034: loss improved from 0.87046 to 0.80448, saving model to weights-improvement-34-0.8045-bigger.hdf5\n",
      "114/114 [==============================] - 17s 152ms/step - loss: 0.8045\n",
      "Epoch 35/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.7425\n",
      "Epoch 00035: loss improved from 0.80448 to 0.74254, saving model to weights-improvement-35-0.7425-bigger.hdf5\n",
      "114/114 [==============================] - 17s 146ms/step - loss: 0.7425\n",
      "Epoch 36/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.7013\n",
      "Epoch 00036: loss improved from 0.74254 to 0.70125, saving model to weights-improvement-36-0.7013-bigger.hdf5\n",
      "114/114 [==============================] - 17s 148ms/step - loss: 0.7013\n",
      "Epoch 37/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.6391\n",
      "Epoch 00037: loss improved from 0.70125 to 0.63913, saving model to weights-improvement-37-0.6391-bigger.hdf5\n",
      "114/114 [==============================] - 17s 152ms/step - loss: 0.6391\n",
      "Epoch 38/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.6073\n",
      "Epoch 00038: loss improved from 0.63913 to 0.60734, saving model to weights-improvement-38-0.6073-bigger.hdf5\n",
      "114/114 [==============================] - 16s 140ms/step - loss: 0.6073\n",
      "Epoch 39/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.5383\n",
      "Epoch 00039: loss improved from 0.60734 to 0.53830, saving model to weights-improvement-39-0.5383-bigger.hdf5\n",
      "114/114 [==============================] - 18s 154ms/step - loss: 0.5383\n",
      "Epoch 40/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.5063\n",
      "Epoch 00040: loss improved from 0.53830 to 0.50628, saving model to weights-improvement-40-0.5063-bigger.hdf5\n",
      "114/114 [==============================] - 17s 146ms/step - loss: 0.5063\n",
      "Epoch 41/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4751\n",
      "Epoch 00041: loss improved from 0.50628 to 0.47508, saving model to weights-improvement-41-0.4751-bigger.hdf5\n",
      "114/114 [==============================] - 16s 143ms/step - loss: 0.4751\n",
      "Epoch 42/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4349\n",
      "Epoch 00042: loss improved from 0.47508 to 0.43494, saving model to weights-improvement-42-0.4349-bigger.hdf5\n",
      "114/114 [==============================] - 17s 145ms/step - loss: 0.4349\n",
      "Epoch 43/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.0021\n",
      "Epoch 00043: loss did not improve from 0.43494\n",
      "114/114 [==============================] - 17s 151ms/step - loss: 1.0021\n",
      "Epoch 44/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 2.1357\n",
      "Epoch 00044: loss did not improve from 0.43494\n",
      "114/114 [==============================] - 17s 146ms/step - loss: 2.1357\n",
      "Epoch 45/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.9833\n",
      "Epoch 00045: loss did not improve from 0.43494\n",
      "114/114 [==============================] - 19s 165ms/step - loss: 0.9833\n",
      "Epoch 46/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.6391\n",
      "Epoch 00046: loss did not improve from 0.43494\n",
      "114/114 [==============================] - 18s 154ms/step - loss: 0.6391\n",
      "Epoch 47/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4905\n",
      "Epoch 00047: loss did not improve from 0.43494\n",
      "114/114 [==============================] - 16s 143ms/step - loss: 0.4905\n",
      "Epoch 48/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.4155\n",
      "Epoch 00048: loss improved from 0.43494 to 0.41553, saving model to weights-improvement-48-0.4155-bigger.hdf5\n",
      "114/114 [==============================] - 18s 154ms/step - loss: 0.4155\n",
      "Epoch 49/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 1.5405\n",
      "Epoch 00049: loss did not improve from 0.41553\n",
      "114/114 [==============================] - 17s 145ms/step - loss: 1.5405\n",
      "Epoch 50/50\n",
      "114/114 [==============================] - ETA: 0s - loss: 3.4354\n",
      "Epoch 00050: loss did not improve from 0.41553\n",
      "114/114 [==============================] - 16s 143ms/step - loss: 3.4354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d59479bd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
